{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh',\n",
       "       'exng', 'oldpeak', 'slp', 'caa', 'thall', 'output'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (302, 14)\n",
      "Filtered DataFrame shape: (230, 14)\n"
     ]
    }
   ],
   "source": [
    "# Specify the columns for which you want to drop outliers\n",
    "columns_to_filter = ['cp', 'trtbps', 'chol', 'fbs', 'restecg','thalachh','exng','caa']\n",
    "\n",
    "# Calculate the first and third quartiles (Q1 and Q3) for each specified column\n",
    "Q1 = df[columns_to_filter].quantile(0.25)\n",
    "Q3 = df[columns_to_filter].quantile(0.75)\n",
    "\n",
    "# Calculate the IQR (Interquartile Range) for each specified column\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for each specified column\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the DataFrame to exclude outliers in the specified columns\n",
    "filtered_df = df.copy()\n",
    "for column in columns_to_filter:\n",
    "    filtered_df = filtered_df[(filtered_df[column] >= lower_bound[column]) & (filtered_df[column] <= upper_bound[column])]\n",
    "\n",
    "# Display the shape of the original and filtered DataFrames\n",
    "print(\"Original DataFrame shape:\", df.shape)\n",
    "print(\"Filtered DataFrame shape:\", filtered_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['sex','age','output']\n",
    "Xall = filtered_df.drop(col, axis = 1)\n",
    "yall = filtered_df['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, Xt, y, yt = train_test_split(Xall, yall, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scl = MinMaxScaler().fit(X)\n",
    "X = scl.transform(X)\n",
    "Xt = scl.transform(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.838):\n",
      "{'lr__C': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe = Pipeline([\n",
    " \n",
    "    ('lr', LogisticRegression(random_state=0))\n",
    "])\n",
    "\n",
    "params = {     \n",
    "   \n",
    "    \"lr__C\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],     \n",
    "}\n",
    "\n",
    "lr_clf = GridSearchCV(pipe, params, n_jobs=2,cv=10)\n",
    "lr_clf.fit(X, y)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % lr_clf.best_score_)\n",
    "print(lr_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9130434782608695"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yp = lr_clf.predict(Xt)\n",
    "lgr = sum(yp == yt) / len(yt)\n",
    "lgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('lr_model.pkl', 'wb') as file:\n",
    "    pickle.dump(lr_clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:    cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  caa  thall\n",
      "0   1     130   250    0        1       150     0      2.5    2    1      3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soill\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the pre-trained model using pickle\n",
    "with open('models/lr_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "# Features for testing\n",
    "test_features = {\n",
    "    'cp': [1],\n",
    "    'trtbps': [130],\n",
    "    'chol': [250],\n",
    "    'fbs': [0],\n",
    "    'restecg': [1],\n",
    "    'thalachh': [150],\n",
    "    'exng': [0],\n",
    "    'oldpeak': [2.5],\n",
    "    'slp': [2],\n",
    "    'caa': [1],\n",
    "    'thall': [3],\n",
    "}\n",
    "\n",
    "# Create a DataFrame with the test features\n",
    "test_data = pd.DataFrame(test_features)\n",
    "\n",
    "# Use the model to make predictions based on the test features\n",
    "prediction = model.predict(test_data)[0]\n",
    "\n",
    "# Display the prediction\n",
    "if prediction == 0:\n",
    "    result = 'No Heart Attack'\n",
    "else:\n",
    "    result = 'Heart Attack'\n",
    "\n",
    "print('Prediction:', test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: scikit-learnNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Version: 1.3.0\n",
      "Summary: A set of python modules for machine learning and data mining\n",
      "Home-page: http://scikit-learn.org\n",
      "Author: \n",
      "Author-email: \n",
      "License: new BSD\n",
      "Location: c:\\users\\soill\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\n",
      "Requires: joblib, scipy, threadpoolctl, numpy\n",
      "Required-by: sklearn\n"
     ]
    }
   ],
   "source": [
    "pip show scikit-learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
